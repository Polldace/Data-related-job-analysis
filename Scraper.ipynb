{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from time import time \n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the file name for save web data\n",
    "file_name = 'jobdata2.xlsx'\n",
    "\n",
    "# Set page numbers for scraping\n",
    "start_page = 151\n",
    "end_page = 200\n",
    "\n",
    "\n",
    "# define the search condition, url link and head\n",
    "search_word = '/data'\n",
    "main_url = \"******\"\n",
    "if start_page == 1:\n",
    "    seek_url = main_url + search_word + \"-jobs\"\n",
    "else:\n",
    "    seek_url = main_url + search_word + \"-jobs\" + '?page='+ str(start_page)\n",
    "header = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"}\n",
    "    \n",
    "# Making a get request to fetch the first webpage \n",
    "seek_page = requests.get(seek_url, headers = header)\n",
    "# Parsing the first webpage using BeautifulSoup\n",
    "seek_soup = bs(seek_page.text,\"lxml\")\n",
    "\n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "count = 1\n",
    "\n",
    "for page in range(start_page,end_page+1):\n",
    "    print(seek_url)\n",
    "    # A list to save all searched jobs information: \n",
    "    #job_url, job title, company, job location, salary and job description\n",
    "    job_data = []\n",
    "    \n",
    "    flag = 0\n",
    "    ## Locating the position of every searched results\n",
    "    all_jobs = seek_soup.find_all(\"div\",attrs={\"data-search-sol-meta\":True})\n",
    "\n",
    "    for job in all_jobs:\n",
    "        job_url = main_url + job.h1.a[\"href\"]\n",
    "        job_title = job.h1.a.text\n",
    "        try:\n",
    "            job_company = job.find(\"a\",attrs={\"data-automation\":\"jobCompany\"}).text\n",
    "        except:\n",
    "            job_company = None\n",
    "\n",
    "        job_location = job.find(text=re.compile(\"location\"))\n",
    "        try:\n",
    "            job_salary = job.find(\"span\",attrs={\"data-automation\":\"jobSalary\"})[\"aria-label\"]\n",
    "        except:\n",
    "            job_salary = None\n",
    "        job_data.append([job_url, job_title, job_company,job_location,job_salary])\n",
    "        print(flag)\n",
    "        flag +=1\n",
    "        \n",
    "\n",
    "    for result in job_data:\n",
    "        # Fetching each job page, to get detailed job info\n",
    "        job_page = requests.get(result[0],timeout = 10, headers = header)\n",
    "        # extract\n",
    "        job_soup = bs(job_page.text,\"lxml\") \n",
    "\n",
    "        # Job published date, industry and job infromation\n",
    "        job_date = job_soup.find_all(\"span\",class_=\"lwHBT6d\")[1].text\n",
    "        job_indus = job_soup.select(\"dl strong\")[1].text\n",
    "        job_info = job_soup.find(\"div\",attrs={\"data-automation\":\"mobileTemplate\"}).text\n",
    "        result.append(job_date)\n",
    "        result.append(job_indus)\n",
    "        result.append(job_info)\n",
    "            # Pause the loop\n",
    "        sleep(random.randint(2,10))\n",
    "    \n",
    "    # Convert data to Pandas format\n",
    "    column = ['job_url','job_title', 'job_company','job_location','job_salary','job_date','job_indus', 'job_info']\n",
    "    job_df = pd.DataFrame(job_data, columns = column)\n",
    "\n",
    "    # Write into excel\n",
    "    # Create a Pandas Excel writer \n",
    "    writer = pd.ExcelWriter(file_name,engine=None,mode = 'a')\n",
    "    sheetname = 'Sheet'+ str(page)\n",
    "\n",
    "     # Write the dataframe with all information of one webpage to one sheet of an XlsxWriter Excel object.\n",
    "    job_df.to_excel(writer, sheet_name=sheetname)\n",
    "     # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.save()\n",
    "        \n",
    "    # Pause the loop\n",
    "    sleep(random.randint(1,8))\n",
    "    \n",
    "    # Monitor the requests\n",
    "    count += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(count, count/elapsed_time))\n",
    "#     clear_output(wait = True)\n",
    "    print('page: ', page)\n",
    "    \n",
    "         \n",
    "    # Update the search page\n",
    "    seek_url = main_url + search_word + \"-jobs\"  + '?page='+ str(page+1)\n",
    "    \n",
    "    # Get the next page and parse the page, Throw a warning for non-200 status codes\n",
    "    try:\n",
    "        seek_page = requests.get(seek_url, headers = header)\n",
    "        if seek_page.status_code == 200:\n",
    "            # extract\n",
    "            seek_soup = bs(seek_page.text,\"lxml\") \n",
    "            print('update page')\n",
    "        else:\n",
    "            print(seek_page.status_code)\n",
    "            # notify, try again\n",
    "    except requests.Timeout as e:\n",
    "        print(\"It is time to timeout\")\n",
    "        print(str(e))\n",
    "\n",
    "    clear_output(wait = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
